{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTRODUCING AZURE MACHINE LEARNING\n",
    "Data often has a story to tell and if you have a lot there are bound to be patterns. It may be too complex for us to detect this patterns ourselves this is where machine learning comes in! It allows us to dump large ammounts of data and let the machine find patterns for us, allow for us to get better products. Machine learning can help us create smarter applications.\n",
    "\n",
    "Say that we are we would like to determine with a high degree of accuracy if a credit card transaction is fraudulent. One option would be to get a few smart people together in a room and think about it then write up code that implements whatever they come up with. This is the most common approach to creating software solutions today. Alternatively if we have data available about the problem we might in just use the data to train a model to figure out an effective solution. In this day in age as data is doubling every year and becoming more accessable simply there will be a shift from using the former option to the latter option. \n",
    "\n",
    "With using data we may just look at it and plot it to find patterns while the cons are that if we have too small of a sample size the patterns we find will likely be wrong. \n",
    "\n",
    "|Name |Amount| Fraudulent|\n",
    "|------|------|------|\n",
    "|Tang  |\\$100 | No|\n",
    "|Smith |\\$200 | Yes  |\n",
    "|Lee | \\$300  | Yes  |\n",
    "|Thomas| \\$100 | No |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the table above we may conclude that anytime a transaction is \\$100 or that the anyones name which does not start with a 'T' is Fradulent. Now imagine if we had 10 million records or transactions and we wanted to pick out patterns for Fradulent trancs actions, it would manually take you too long to find a pattern and check it witht he rest of the data to make sure it works. Luckily we have machines that can do this for us and with the help of statistical techniques we can find patterns in these large data sets. So our model is created through code but it must be fed information with the correct fields. Only then can our model be able to well if an observation is likely to be fraudulent or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
