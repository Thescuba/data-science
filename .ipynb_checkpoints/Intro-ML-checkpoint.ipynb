{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning\n",
    "## What is Machine Learning?\n",
    "Machine learning is a way of using data to train a computer model so taht it can predict new data values based on known inputs. The model learns from training cases and we use the trained model to make predictions for new data cases. Computer are very good at performing calculation so we just need to find a way to train a computer to perform the correct calculations. To start we need dataset that contrains historical records often called cases or observations. Ideally we would like each observation to include numberic features that quantify a characteristic of the item we are working with. In other words each observation is a vector that has a measurement for each feature and dataset of all these observations is what we refer to as X. In general we will also have value we are trying to predict which we will call Y.\n",
    "\n",
    "So basically our goal is to output solve for the function which operates on a set of X and produces prediction Y.\n",
    "# $f(X) = Y$\n",
    "\n",
    "\n",
    "Generally in machine learning there are two borad kinds of machine learning; supervised and unsupervised. In supervised learn we have known values we want to predict these are called labels or Y. Given that we have labels we can check and validate our model by comparing the values predicted to the actual label value that we had in the first place. We train and tune parameters until we are happy with the performance of our model and then we test it on new observations for which the label is unknown and generate a new prediction for the value. Unsupervised learning is different from supervised learning in that we are not given labels in our training data. We train the model by finding similarities in the observation and after the model is trained each new observation is assigned a cluster, or a group in which they had similar characteristics.<br>\n",
    "\n",
    "### Supervised Learning \n",
    "![](http://i.imgur.com/LIpP1Fu.png5)<br>\n",
    "### Unsupervised Learning \n",
    "![](http://i.imgur.com/ecAmN30.png)<br>\n",
    "### General Machine Learning\n",
    "![](https://ww2.mathworks.cn/help/stats/machinelearning_supervisedunsupervised.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions\n",
    "Supervised learning splits into to cases, the first one we will talk aobut is regesssion. Give our data with labels we need to predict if our labels are continous or discrete, if they are continuous we want to use the regession approach. In our example suppose rosie has some historic lemonade sales data and she wants to use machine learning to predict how many lemonade she might sell on a given day. We can see that our sales data is a numeric that is continuous so regession would be the best option here. Lets supppose Rosie had gather her historic sales data which includes the temperature, rainfall, flyers and sales. Lets take an example observation to see what we are trying to do.<br>\n",
    "![](images/model.png)\n",
    "\n",
    "We need our algorithm to learn a function that operates on all of the features to give us a resulting label of 12.\n",
    "\n",
    "Note that a sample from a single dont wont allow our function to generalize well so we gather the same data over multiple days and train our model based on the larger set. Once our model is trained with the data from the training set we can interpolate for any new value of X to predict a new Y.\n",
    "\n",
    "It if a good idea to hold some of the data we have out of the training set because the model is attempting to fit the data as best as it could but that may come at the cost of overfitting or have a model that is too specific to the data and does not generalize well. We can use the data that we withheld to evaluate the models performance. The data that is withheld is often refered to as the testing data. We can then input our testing data into our trained model and returnt he results as $\\hat{y}$ which we want to compare with the labels of our testing data and meassure its performance. The most common perforance metric we use is root mean squared errors or RMSE. \n",
    "\n",
    "### $RMSE = \\sqrt{\\sum_{}{}(\\hat{y} - label)^{2}}$\n",
    "\n",
    "Another metric that is also often use is the mean absolute error.\n",
    "\n",
    "### $MAE = \\frac{\\sum_{}{}abs(\\hat{y} - label)}{n}$\n",
    "\n",
    "We can use relative performance metrics to see how well or model performs, while RMSE may return a value of 17, we may need to figure out what the value of 17 means, is that good or is it bad? We can introduce two relative metrics such as relative absolute error relative to the mean value of the label. RAE will turn a value between 0 and 1 where 0 is a perfect model. Relative measurement are a useful way to generally evaluate the performance of a model. \n",
    "\n",
    "### $\\mathrm{ RAE} = \\frac{ \\sum_{}{} abs( \\hat{y} - label) } {  \\sum_{}{} label }$\n",
    "\n",
    "Another relative metric is relative squared error, which is the RMSE divided by the sum of the squares of the label.\n",
    "\n",
    "### $\\mathrm{ RSE} = \\frac{ \\sqrt{\\sum_{}{} abs( \\hat{y} - label)^{2}} } {  \\sum_{}{}label^{2} }$\n",
    "\n",
    "Finally we have the coefficient of determination also know as the $R^{2}$ of teh model represent the predictive power of the model as a value between 0 and 1. Where a value of 0 means the model is random and had learn nothing and a perfect fit would result in a value of 1.\n",
    "\n",
    "\n",
    "### Classification\n",
    "We have seen how to train a regression model but what would happen if our labels were discrete? We would want to use a classification model, to get the basics down we will talk about binary classification, where we only have two values in which the label can take. Often when working with biniary classification we want to check if something is true or false, say in the case of a medical test the two results can be you have a food allergie or you dont have a food allergie, this can be simplfied into the question of having a food allergy is true or false.\n",
    "\n",
    "In our exmples we will see if Rosie keeps a record of the temperture, rainfall, flyers and profits her lemonaid stand makes. Profits is defined by if she made profit that day or not. <br>\n",
    "![](images/model2.png)\n",
    "\n",
    "The model will not automatically assign these values you a 0 or a 1 but will use a threshold to assign them 0 or a 1. The figure below has the threshold set at 0.5 <br>\n",
    "![](images/model3.png)\n",
    "\n",
    "In the example above we see the actual labels  represent by red X for fales and a green check for positive, our threshold is at 0.5 so our model predicts that anything to the right of the threshold is True and everything to the left is a negitive. Before our perforance was measure by the error that our results return in classification we have 4 possible outcomes true positive, false positive, true negitive and false negitive. \n",
    "\n",
    "### <center>Confusion Matrix</center>\n",
    "\n",
    "|Label |True| False|\n",
    "|------|------|------|\n",
    "|True  |True Positive  | False Positive   |\n",
    "|False | False Negitive  | True Negitive   |\n",
    "\n",
    "A predicted value may be very close to the threshhold and may be classified wrong and it the job of the data scientist to figure out what values to use for the threshold. The results from true positive, false positive, true negitive and false negitive all play a big part in calculating the performance metrix of a classifier. For us the simplest metric is accuracy which is defined by the sum of all right predictions over the total number of cases.\n",
    "\n",
    "#### $ Accuracy = (TP +TN)/ (TP + FP +TN + FN)$ \n",
    "\n",
    "Accuracy however is not always the a good measurement of performance. Suppose we have a rare disease and we find that 1 in 1000 people have this disease, if we wanted to get a model with good accuracy we could just have are model say that none has this disease, our model will only be wrong 1 out every 1000 test so we would still have a 99.9 percent accuracy, but the test itself providing no value, it would be pointless to even take this test. \n",
    "\n",
    "A more useful metric to use if precision which is all true positives divided by all positive guesses. Precesion represents the chance that given a positive that it is a true positive.\n",
    "\n",
    "#### $ Precision = (TP)/ (TP + FP)$\n",
    "\n",
    "Recall is what fractions of positive cases are correctly identified.\n",
    "#### $ Recall = (TP)/ (TP + FN) = True Positive Rate$\n",
    "There is also a false positive rate \n",
    "#### $ False Positive Rate = (FP)/ (FP + TN)$\n",
    "\n",
    "\n",
    "## Unsupervised Learning\n",
    "In and unsupervised learning problem we our training data lacks labels. Eventhough our data lacks labels we are still able to perform analysis on our data by checkout out how similar two items are and grouping them into clusters. Suppose for example Rosie would like to segment her customers into groups based on age, how often they show up and the ammount they spend. One of the most common clustering techniques is k-means. \n",
    "\n",
    "### How does the k means algorithm work? \n",
    "For a high level we start off with assigning k to the number of cluster we would like, then we assign k random points be our centroid data for our clusters. We go through each of our data and assign each data point to the closest centroid. After all points have been assigned we then update each centroid with the meean value of all the other centroids and then reassign our data to the closest centroid once again. This process repeast until we converge or the centroids no longer change. Centroids are represented as stars and the dots are our observations.\n",
    "#### Initialize Centroids \n",
    "![](images/unsuper0.png)\n",
    "#### Assign Data to a Cluster or Centroid\n",
    "![](images/unsuper00.png)\n",
    "#### Update Centroid\n",
    "![](images/unsuper.png)\n",
    "\n",
    "\n",
    "### How do do we measure how good a cluster is?\n",
    "#### One option is to compare the average distance bwtween the cluster centers and the points in their clusters<br>\n",
    "![](images/cluster.png)<br>\n",
    "#### Another is One option is to compare the average distance bwtween the cluster centers and the point that is maximum distance between the points and the centroid of the clusters.<br>\n",
    "![](images/cluster2.png)\n",
    "\n",
    "#### We can also use principal component analysis on our cluster to figure the direction in represents maximum variance, which is represented by the major axis of the ellipse while the second principal component is the minor axis of the ellipse. \n",
    "![](images/ellipse.png)\n",
    "\n",
    "#### Running PCA on our second cluster will return a second ellipse and if it looks like the figure below we can see that the major axis are perpendicular meaning that the our cluster are nicely separated\n",
    "![](images/ellipse1.png)\n",
    "\n",
    "#### If our data is colinear like in the figure below we know that our cluster may not be as nicely separated and the major axis of the second ellipse is less well defined. \n",
    "![](images/ellipse3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
